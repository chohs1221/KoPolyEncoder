============================================================
File Name: poly221215_0912_bs128_ep10_data131131_ko
START!! 2022_12_15 / 09_12
model: poly
path: skt/kobert-base-v1
trainset: ko_train_131131
validset: ko_valid_16391
m: 16
seed: 42
epoch: 10
learning rate: 5e-05
batch size: 128
accumulation: 1
max length: 40
language: ko
scheduler: True
description: 

train: 131131
valid: 16391
['이번 프로젝트에서 발표를 하는데 내가 실수하는 바람에 우리 팀이 감점을 받았어. 너무 미안해.', '내 능력이 부족한 거 같은데 그만 다녀야 될거같아.', '퇴근 후 여가에 회사 일을 더 열심히 해서 피해가 가지 않도록 해야겠어.', '회사에서 중요한 프로젝트를 혼자 하게 됐는데 솔직히 두렵고 무서워.', '나에게 너무 크게 느껴지는 중요한 프로젝트라 버거운 느낌이 들어.']
['실수하시다니 정말 미안한 마음이 크겠어요.', '능력을 올리려면 어떤 방법이 있을까요?', '꼭 좋은 결과 있길 바라요.', '큰 프로젝트를 혼자 하셔서 고민이 많겠네요.', '프로젝트를 잘하시기 위해서 어떤 걸 할 수 있나요?']

train loss: 4.481370932915631 / valid loss: 2.8620001263916492 -------------------- epoch: 0 iteration: 102 ==> save
train loss: 3.5052739685656977 / valid loss: 2.6050010677427053 -------------------- epoch: 0 iteration: 204 ==> save
train loss: 2.90824420078128 / valid loss: 2.265710008330643 -------------------- epoch: 0 iteration: 306 ==> save
train loss: 2.664585267796236 / valid loss: 2.142488199286163 -------------------- epoch: 0 iteration: 408 ==> save
train loss: 2.571218083886539 / valid loss: 2.1228622319176793 -------------------- epoch: 0 iteration: 510 ==> save
scheduler!
train loss: 2.5054033083074234 / valid loss: 2.083835819736123 -------------------- epoch: 0 iteration: 612 ==> save
train loss: 2.549117791886423 / valid loss: 2.034863575361669 -------------------- epoch: 0 iteration: 714 ==> save
train loss: 2.4376706539415847 / valid loss: 2.0178473396226764 -------------------- epoch: 0 iteration: 816 ==> save
train loss: 2.415221225981619 / valid loss: 1.9874296737834811 -------------------- epoch: 0 iteration: 918 ==> save
train loss: 2.3675802385105804 / valid loss: 1.9865734949707985 -------------------- epoch: 0 iteration: 1020 ==> save
scheduler!
train loss: 2.3342130967215 / valid loss: 1.9462639223784208 -------------------- epoch: 1 iteration: 102 ==> save
train loss: 2.1961611778128383 / valid loss: 1.9920726707205176 -------------------- epoch: 1 iteration: 204
train loss: 2.1759388458495046 / valid loss: 2.0438845800235868 -------------------- epoch: 1 iteration: 306
train loss: 2.141337986085929 / valid loss: 1.9643970746546984 -------------------- epoch: 1 iteration: 408
train loss: 2.17958255258261 / valid loss: 1.992944031022489 -------------------- epoch: 1 iteration: 510
scheduler!
train loss: 2.1750481374123516 / valid loss: 1.9455805867910385 -------------------- epoch: 1 iteration: 612 ==> save
train loss: 2.162599661770989 / valid loss: 1.9526061741635203 -------------------- epoch: 1 iteration: 714
train loss: 2.1373186111450195 / valid loss: 1.8890393422916532 -------------------- epoch: 1 iteration: 816 ==> save
train loss: 2.1618958512941995 / valid loss: 1.9011859772726893 -------------------- epoch: 1 iteration: 918
train loss: 2.1102196013226227 / valid loss: 1.8849240699782968 -------------------- epoch: 1 iteration: 1020 ==> save
scheduler!
train loss: 1.9976123466211206 / valid loss: 1.9340990372002125 -------------------- epoch: 2 iteration: 102
train loss: 1.9055354174445658 / valid loss: 1.967399070970714 -------------------- epoch: 2 iteration: 204
train loss: 1.9223088341600754 / valid loss: 1.9106262251734734 -------------------- epoch: 2 iteration: 306
train loss: 1.9373077761893178 / valid loss: 1.9219364887103438 -------------------- epoch: 2 iteration: 408
train loss: 1.9348854668000166 / valid loss: 1.8930082339793444 -------------------- epoch: 2 iteration: 510
scheduler!
train loss: 1.9190888182789672 / valid loss: 1.8646281557157636 -------------------- epoch: 2 iteration: 612 ==> save
train loss: 1.930293397576201 / valid loss: 1.933347281999886 -------------------- epoch: 2 iteration: 714
train loss: 1.9117976216708912 / valid loss: 1.899865229614079 -------------------- epoch: 2 iteration: 816
train loss: 1.896119551331389 / valid loss: 1.869002659805119 -------------------- epoch: 2 iteration: 918
train loss: 1.9481972561163061 / valid loss: 1.949722713790834 -------------------- epoch: 2 iteration: 1020
scheduler!
train loss: 1.7735793765853434 / valid loss: 1.9371028868481517 -------------------- epoch: 3 iteration: 102
train loss: 1.711887100163628 / valid loss: 2.005276850424707 -------------------- epoch: 3 iteration: 204
train loss: 1.7411951971989053 / valid loss: 1.94411816354841 -------------------- epoch: 3 iteration: 306
train loss: 1.7437176517411774 / valid loss: 1.949096331372857 -------------------- epoch: 3 iteration: 408
train loss: 1.7358143119251026 / valid loss: 1.9112489856779575 -------------------- epoch: 3 iteration: 510
scheduler!
train loss: 1.7542764462676703 / valid loss: 2.016180365346372 -------------------- epoch: 3 iteration: 612
train loss: 1.7330063020481783 / valid loss: 1.9438666785135865 -------------------- epoch: 3 iteration: 714
train loss: 1.741785449140212 / valid loss: 2.023699608631432 -------------------- epoch: 3 iteration: 816
train loss: 1.727145279155058 / valid loss: 1.9535966478288174 -------------------- epoch: 3 iteration: 918
train loss: 1.7309605292245454 / valid loss: 1.9748757798224688 -------------------- epoch: 3 iteration: 1020
scheduler!
train loss: 1.4930377053279502 / valid loss: 2.047038124874234 -------------------- epoch: 4 iteration: 102
train loss: 1.5043440513751085 / valid loss: 2.058400698006153 -------------------- epoch: 4 iteration: 204
train loss: 1.547153700800503 / valid loss: 2.0370367569848895 -------------------- epoch: 4 iteration: 306
train loss: 1.5199982687538744 / valid loss: 2.091673423536122 -------------------- epoch: 4 iteration: 408
train loss: 1.5526422916674147 / valid loss: 2.032097208313644 -------------------- epoch: 4 iteration: 510
scheduler!
train loss: 1.5417938781719582 / valid loss: 1.9957891963422298 -------------------- epoch: 4 iteration: 612
train loss: 1.576772291286319 / valid loss: 2.010375593788922 -------------------- epoch: 4 iteration: 714
train loss: 1.56241277736776 / valid loss: 2.0194363901391625 -------------------- epoch: 4 iteration: 816
train loss: 1.5604244680965649 / valid loss: 2.0221058325842023 -------------------- epoch: 4 iteration: 918
train loss: 1.5829009250098585 / valid loss: 2.0035916585475206 -------------------- epoch: 4 iteration: 1020
scheduler!
train loss: 1.3315775230819105 / valid loss: 2.1338862907141447 -------------------- epoch: 5 iteration: 102
train loss: 1.307889029675839 / valid loss: 2.12701918464154 -------------------- epoch: 5 iteration: 204
train loss: 1.3426098227500916 / valid loss: 2.1552911903709173 -------------------- epoch: 5 iteration: 306
train loss: 1.375344566270417 / valid loss: 2.0660655461251736 -------------------- epoch: 5 iteration: 408
train loss: 1.34369169847638 / valid loss: 2.1358697693794966 -------------------- epoch: 5 iteration: 510
scheduler!
train loss: 1.3823333955278583 / valid loss: 2.0792696634307504 -------------------- epoch: 5 iteration: 612
train loss: 1.3905409644631779 / valid loss: 2.140289262868464 -------------------- epoch: 5 iteration: 714
train loss: 1.4090627992854399 / valid loss: 2.1218139147385955 -------------------- epoch: 5 iteration: 816
train loss: 1.3896396475679733 / valid loss: 2.154267229139805 -------------------- epoch: 5 iteration: 918
train loss: 1.4048257236387216 / valid loss: 2.073658924549818 -------------------- epoch: 5 iteration: 1020
scheduler!
train loss: 1.1623605928000282 / valid loss: 2.176425655372441 -------------------- epoch: 6 iteration: 102
train loss: 1.1344920297463734 / valid loss: 2.219896257854998 -------------------- epoch: 6 iteration: 204
train loss: 1.1851393089574926 / valid loss: 2.2166888592764735 -------------------- epoch: 6 iteration: 306
train loss: 1.152371687631981 / valid loss: 2.228765449486673 -------------------- epoch: 6 iteration: 408
train loss: 1.1771312005379622 / valid loss: 2.278927944600582 -------------------- epoch: 6 iteration: 510
scheduler!
train loss: 1.203034776098588 / valid loss: 2.225309639237821 -------------------- epoch: 6 iteration: 612
train loss: 1.216588323022805 / valid loss: 2.2610095404088497 -------------------- epoch: 6 iteration: 714
train loss: 1.2290251740053588 / valid loss: 2.2773535260930657 -------------------- epoch: 6 iteration: 816
train loss: 1.2338179157060736 / valid loss: 2.205699988640845 -------------------- epoch: 6 iteration: 918
train loss: 1.255361811203115 / valid loss: 2.2628980660811067 -------------------- epoch: 6 iteration: 1020
scheduler!
train loss: 0.9915779647873897 / valid loss: 2.3687301706522703 -------------------- epoch: 7 iteration: 102
train loss: 0.9843654574132433 / valid loss: 2.3974987883120775 -------------------- epoch: 7 iteration: 204
train loss: 1.0125657214837915 / valid loss: 2.3782432107254863 -------------------- epoch: 7 iteration: 306
train loss: 1.0134732559615491 / valid loss: 2.3511483995243907 -------------------- epoch: 7 iteration: 408
train loss: 1.028487835444656 / valid loss: 2.386143716983497 -------------------- epoch: 7 iteration: 510
scheduler!
train loss: 0.9831471863914939 / valid loss: 2.308442547917366 -------------------- epoch: 7 iteration: 612
train loss: 0.9272550256813273 / valid loss: 2.338981612585485 -------------------- epoch: 7 iteration: 714
train loss: 0.9582081583200717 / valid loss: 2.3290635980665684 -------------------- epoch: 7 iteration: 816
train loss: 0.919188400109609 / valid loss: 2.30151748098433 -------------------- epoch: 7 iteration: 918
train loss: 0.9065334452133552 / valid loss: 2.326224426738918 -------------------- epoch: 7 iteration: 1020
scheduler!
train loss: 0.7193146783347223 / valid loss: 2.423680198378861 -------------------- epoch: 8 iteration: 102
train loss: 0.6956148448528028 / valid loss: 2.4371560849249363 -------------------- epoch: 8 iteration: 204
train loss: 0.7172238844282487 / valid loss: 2.4039941914379597 -------------------- epoch: 8 iteration: 306
train loss: 0.7072148124376932 / valid loss: 2.4121725792065263 -------------------- epoch: 8 iteration: 408
train loss: 0.7011786483082116 / valid loss: 2.475317937321961 -------------------- epoch: 8 iteration: 510
scheduler!
train loss: 0.7066950126021516 / valid loss: 2.417318847961724 -------------------- epoch: 8 iteration: 612
train loss: 0.7264888821279302 / valid loss: 2.437817458063364 -------------------- epoch: 8 iteration: 714
train loss: 0.7101559545479569 / valid loss: 2.439991746097803 -------------------- epoch: 8 iteration: 816
train loss: 0.7385209500789642 / valid loss: 2.4986770413815975 -------------------- epoch: 8 iteration: 918
train loss: 0.7213252736072914 / valid loss: 2.5423040371388197 -------------------- epoch: 8 iteration: 1020
scheduler!
train loss: 0.6016594133540696 / valid loss: 2.538388912566006 -------------------- epoch: 9 iteration: 102
train loss: 0.5904759843559826 / valid loss: 2.6052102306857705 -------------------- epoch: 9 iteration: 204
train loss: 0.5904385952972898 / valid loss: 2.5577404806390405 -------------------- epoch: 9 iteration: 306
train loss: 0.6125090925716886 / valid loss: 2.590309855528176 -------------------- epoch: 9 iteration: 408
train loss: 0.6082019876031315 / valid loss: 2.5902339667081833 -------------------- epoch: 9 iteration: 510
scheduler!
train loss: 0.6246631043214425 / valid loss: 2.553676691837609 -------------------- epoch: 9 iteration: 612
train loss: 0.6039110568224215 / valid loss: 2.559162002056837 -------------------- epoch: 9 iteration: 714
train loss: 0.6234109597463234 / valid loss: 2.565251267515123 -------------------- epoch: 9 iteration: 816
train loss: 0.6382527564670525 / valid loss: 2.6359273763373494 -------------------- epoch: 9 iteration: 918
train loss: 0.6464504213894114 / valid loss: 2.5310171134769917 -------------------- epoch: 9 iteration: 1020
scheduler!
END!! 2022_12_15 / 11_16
RUNNING TIME: 2:03:45
============================================================



SCORE!!
Namespace(batch=32, best='0', device='cuda', lang='ko', m=16, max_length=40, model='poly', path='poly221215_0912_bs128_ep10_data131131_ko_best0', task='ko', testset='ko_test_16392.pickle')
Load PolyEncoder
poly221215_0912_bs128_ep10_data131131_ko_best0
R@1/100: 58.71
MRR: 67.21
============================================================

Load PolyEncoder
poly221215_0912_bs128_ep10_data131131_ko_best0
R@1/20: 72.93
MRR: 81.56
============================================================

Load PolyEncoder
poly221215_0912_bs128_ep10_data131131_ko_best0
R@1/10: 80.15
MRR: 87.8
============================================================

SCORE!!
Namespace(batch=32, best='1', device='cuda', lang='ko', m=16, max_length=40, model='poly', path='poly221215_0912_bs128_ep10_data131131_ko_best1', task='ko', testset='ko_test_16392.pickle')
Load PolyEncoder
poly221215_0912_bs128_ep10_data131131_ko_best1
R@1/100: 60.42
MRR: 68.93
============================================================

Load PolyEncoder
poly221215_0912_bs128_ep10_data131131_ko_best1
R@1/20: 74.11
MRR: 82.66
============================================================

Load PolyEncoder
poly221215_0912_bs128_ep10_data131131_ko_best1
R@1/10: 81.45
MRR: 88.77
============================================================

