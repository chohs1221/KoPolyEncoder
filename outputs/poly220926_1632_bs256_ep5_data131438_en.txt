============================================================
File Name: poly220926_1632_bs256_ep5_datatrain_131438_en
START!! 2022_09_26 / 16_32
model: poly
path: bert-base-uncased
trainset: persona_train_131438
validset: persona_valid_7801
m: 360
seed: 42
epoch: 5
learning rate: 5e-05
batch size: 256
accumulation: 1
language: en
description: 

train: 131438
valid: 7801
["Hi, how are you doing? I'm getting ready to do some cheetah chasing to stay in shape.", 'I am! For my hobby I like to do canning or some whittling.', "That's neat. When I was in high school I placed 6th in 100m dash!", 'I do not. But I do have a favorite meat since that is all I eat exclusively.', 'I would have to say its prime rib. Do you have any favorite foods?']
['You must be very fast. Hunting is one of my favorite hobbies.', 'I also remodel homes when I am not out bow hunting.', "That's awesome. Do you have a favorite season or time of year?", 'What is your favorite meat to eat?', 'I like chicken or macaroni and cheese.']

train loss: 6.172428551842184 / valid loss: 4.480075438817342 -------------------- epoch: 0 iteration: 51 ==> save
train loss: 4.347166720558615 / valid loss: 3.7845953941345214 -------------------- epoch: 0 iteration: 102 ==> save
train loss: 3.9306283277623795 / valid loss: 3.633113471666972 -------------------- epoch: 0 iteration: 153 ==> save
train loss: 3.612160972520417 / valid loss: 3.3318254152933755 -------------------- epoch: 0 iteration: 204 ==> save
scheduler!
train loss: 3.3200505481046787 / valid loss: 3.0996572335561114 -------------------- epoch: 0 iteration: 255 ==> save
train loss: 3.041443665822347 / valid loss: 2.7544802109400432 -------------------- epoch: 0 iteration: 306 ==> save
train loss: 2.842842840680889 / valid loss: 2.636265277862549 -------------------- epoch: 0 iteration: 357 ==> save
train loss: 2.701794881446689 / valid loss: 2.49296399752299 -------------------- epoch: 0 iteration: 408 ==> save
train loss: 2.552248753753363 / valid loss: 2.3813551982243855 -------------------- epoch: 0 iteration: 459 ==> save
scheduler!
train loss: 2.4704141990811217 / valid loss: 2.3103678385416666 -------------------- epoch: 0 iteration: 510 ==> save
train loss: 2.425158047208599 / valid loss: 2.290456215540568 -------------------- epoch: 1 iteration: 51 ==> save
train loss: 2.1937321845222923 / valid loss: 2.1929130991299948 -------------------- epoch: 1 iteration: 102 ==> save
train loss: 2.132314857314615 / valid loss: 2.1177417914072674 -------------------- epoch: 1 iteration: 153 ==> save
train loss: 2.0629123145458745 / valid loss: 2.000741132100423 -------------------- epoch: 1 iteration: 204 ==> save
scheduler!
train loss: 1.9723209493300493 / valid loss: 1.9340511242548624 -------------------- epoch: 1 iteration: 255 ==> save
train loss: 1.8850443176194733 / valid loss: 1.9140762249628702 -------------------- epoch: 1 iteration: 306 ==> save
train loss: 1.8311815051471485 / valid loss: 1.749080236752828 -------------------- epoch: 1 iteration: 357 ==> save
train loss: 1.7767715196983487 / valid loss: 1.7056259274482728 -------------------- epoch: 1 iteration: 408 ==> save
train loss: 1.701692826607648 / valid loss: 1.6420075297355652 -------------------- epoch: 1 iteration: 459 ==> save
scheduler!
train loss: 1.629273629656025 / valid loss: 1.6289373199144999 -------------------- epoch: 1 iteration: 510 ==> save
train loss: 1.5758318527072084 / valid loss: 1.5545199871063233 -------------------- epoch: 2 iteration: 51 ==> save
train loss: 1.4555878989836748 / valid loss: 1.5357495665550231 -------------------- epoch: 2 iteration: 102 ==> save
train loss: 1.4124287296743954 / valid loss: 1.4611536661783855 -------------------- epoch: 2 iteration: 153 ==> save
train loss: 1.3617169389537735 / valid loss: 1.3778088013331096 -------------------- epoch: 2 iteration: 204 ==> save
scheduler!
train loss: 1.3036961625604069 / valid loss: 1.3412500739097595 -------------------- epoch: 2 iteration: 255 ==> save
train loss: 1.2787192732680077 / valid loss: 1.303844690322876 -------------------- epoch: 2 iteration: 306 ==> save
train loss: 1.2359499650843002 / valid loss: 1.2478826999664308 -------------------- epoch: 2 iteration: 357 ==> save
train loss: 1.1780124622232773 / valid loss: 1.192454425493876 -------------------- epoch: 2 iteration: 408 ==> save
train loss: 1.1362274744931389 / valid loss: 1.1325120727221172 -------------------- epoch: 2 iteration: 459 ==> save
scheduler!
train loss: 1.1049643544589771 / valid loss: 1.0770023425420125 -------------------- epoch: 2 iteration: 510 ==> save
train loss: 1.0446298683390898 / valid loss: 1.086938613653183 -------------------- epoch: 3 iteration: 51
train loss: 0.9276947390799429 / valid loss: 0.9975105583667755 -------------------- epoch: 3 iteration: 102 ==> save
train loss: 0.892241988696304 / valid loss: 1.0689490775267283 -------------------- epoch: 3 iteration: 153
train loss: 0.9225096632452572 / valid loss: 0.9860738595326741 -------------------- epoch: 3 iteration: 204 ==> save
scheduler!
train loss: 0.8708954196350247 / valid loss: 0.9811541259288787 -------------------- epoch: 3 iteration: 255 ==> save
train loss: 0.8589491890925988 / valid loss: 0.965363089243571 -------------------- epoch: 3 iteration: 306 ==> save
train loss: 0.835828140670178 / valid loss: 0.8946548104286194 -------------------- epoch: 3 iteration: 357 ==> save
train loss: 0.8185215791066488 / valid loss: 0.9076195359230042 -------------------- epoch: 3 iteration: 408
train loss: 0.8058369732370564 / valid loss: 0.8846006631851197 -------------------- epoch: 3 iteration: 459 ==> save
scheduler!
train loss: 0.8058683451484231 / valid loss: 0.9026419063409169 -------------------- epoch: 3 iteration: 510
train loss: 0.754123681900548 / valid loss: 0.8278719941775005 -------------------- epoch: 4 iteration: 51 ==> save
train loss: 0.6951023737589518 / valid loss: 0.825644318262736 -------------------- epoch: 4 iteration: 102 ==> save
train loss: 0.7114557740735072 / valid loss: 0.8712441941102346 -------------------- epoch: 4 iteration: 153
train loss: 0.703717409395704 / valid loss: 0.9436194578806559 -------------------- epoch: 4 iteration: 204
scheduler!
train loss: 0.7008366666588128 / valid loss: 0.8020902593930562 -------------------- epoch: 4 iteration: 255 ==> save
train loss: 0.6806775039317561 / valid loss: 0.8126922130584717 -------------------- epoch: 4 iteration: 306
train loss: 0.6824625929196676 / valid loss: 0.8150834083557129 -------------------- epoch: 4 iteration: 357
train loss: 0.678538394909279 / valid loss: 0.8028367956479391 -------------------- epoch: 4 iteration: 408
train loss: 0.655743023928474 / valid loss: 0.8884353876113892 -------------------- epoch: 4 iteration: 459
scheduler!
train loss: 0.677540032302632 / valid loss: 0.8048961003621419 -------------------- epoch: 4 iteration: 510
END!! 2022_09_26 / 17_24
RUNNING TIME: 0:51:21
============================================================



SCORE!!
Namespace(best='0', device='cuda', lang='en', m=360, model='poly', path='poly220926_1632_bs256_ep5_data131438_en_best0', testset='persona_test_7801.pickle')
Load PolyEncoder
poly220926_1632_bs256_ep5_data131438_en_best0
R@1/20: 95.78
MRR: 97.8
============================================================

SCORE!!
Namespace(best='1', device='cuda', lang='en', m=360, model='poly', path='poly220926_1632_bs256_ep5_data131438_en_best1', testset='persona_test_7801.pickle')
Load PolyEncoder
poly220926_1632_bs256_ep5_data131438_en_best1
R@1/20: 95.82
MRR: 97.85
============================================================

SCORE!!
Namespace(best='0', device='cuda', lang='en', m=360, model='poly', path='poly220926_1632_bs256_ep5_data131438_en_best0', testset='persona_test_156020.pickle')
Load PolyEncoder
poly220926_1632_bs256_ep5_data131438_en_best0
R@1/20: 95.6
MRR: 97.75
============================================================

SCORE!!
Namespace(best='1', device='cuda', lang='en', m=360, model='poly', path='poly220926_1632_bs256_ep5_data131438_en_best1', testset='persona_test_156020.pickle')
Load PolyEncoder
poly220926_1632_bs256_ep5_data131438_en_best1
R@1/20: 96.15
MRR: 98.01
============================================================

