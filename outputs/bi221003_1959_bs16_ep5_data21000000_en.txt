============================================================
File Name: bi221003_1959_bs16_ep5_data21000000_en
START!! 2022_10_03 / 19_59
model: bi
path: bert-base-uncased
trainset: ubuntu2_train_1000000
validset: ubuntu2_valid_19560
m: 0
seed: 42
epoch: 5
learning rate: 5e-05
batch size: 16
accumulation: 1
language: en
description: 

train: 1000000
valid: 19560
["i think we could import the old comments via rsync, but from there we need to go via email. I think it is easier than caching the status on each bug and than import bits here and there __eou__ __eot__ it would be very easy to keep a hash db of message-ids  __eou__ sounds good __eou__ __eot__ ok __eou__ perhaps we can ship an ad-hoc apt_prefereces __eou__ __eot__ version? __eou__ __eot__ thanks __eou__ __eot__ not yet __eou__ it is covered by your insurance? __eou__ __eot__ yes __eou__ but it's really not the right time :/ __eou__ with a changing house upcoming in 3 weeks __eou__ __eot__ you will be moving into your house soon? __eou__ posted a message recently which explains what to do if the autoconfiguration does not do what you expect __eou__ __eot__ how urgent is #896? __eou__ __eot__ not particularly urgent, but a policy violation __eou__ __eot__ i agree that we should kill the -novtswitch __eou__ __eot__ ok __eou__ __eot__ would you consider a package split a feature? __eou__ __eot__ context? __eou__ __eot__ splitting xfonts* out of xfree86*. one upload for the rest of the life and that's it __eou__ __eot__ splitting the source package you mean? __eou__ __eot__ yes. same binary packages. __eou__ __eot__ I would prefer to avoid it at this stage.  this is something that has gone into XSF svn, I assume? __eou__ __eot__ ", 'I\'m not suggesting all - only the ones you modify. __eou__ __eot__ ok, it sounds like you\'re agreeing with me, then __eou__ though rather than "the ones we modify", my idea is "the ones we need to merge" __eou__ __eot__ ', "afternoon all __eou__ not entirely related to warty, but if grub-install takes 5 minutes to install, is this a sign that i should just retry the install :) __eou__ __eot__ here  __eou__ __eot__ you might want to know that thinice in warty is buggy compared to that in sid __eou__ __eot__ and apparently GNOME is suddently almost perfect (out of the thinice problem), nobody report bugs :-P __eou__ I don't get your question, where do you want to paste ? __eou__ __eot__ can i file the panel not linking to eds? :) __eou__ __eot__ are you using alt ? or the windows key ? __eou__ wait for the gnome-themes, component will be added __eou__ __eot__ i just restarted X and now nautilus won't show the desktop :( __eou__ hal isn't starting :( __eou__ __eot__ do you think we have any interest to have hal support turned on in gnome-vfs at this point ? It increases the sources of problems for no real benefit imho ... __eou__ __eot__ is it a known bug that g-s-t doesn't know what distribution its running on? __eou__ are there any changes to desktop-file-utils you've got hidden away? __eou__ __eot__ somebody should really kick that guy *hard* __eou__ I've added a build-dep on libxt-dev in warty for zenity __eou__ __eot__ arse. xt-dev? i added libx11-dev __eou__ so just libxt-dev or libxt and libx11? __eou__ for future note, the xmodmap line in that X sticky-super fixes the problem for me __eou__ __eot__ we have planned to speak about menu organisation during the 2 weeks __eou__ I need we don't need to force it __eou__ ? __eou__ __eot__ was away, you said ? __eou__ nope __eou__ __eot__ the warty repository __eou__ ok, fine. Thanks __eou__ nice to get packages update every 30min instead once a day, isn't it :) __eou__ __eot__ you'll be glad to know i've fixed my missing arrows in thinice bug __eou__ __eot__ I've uploaded the gnome-vfs without hal support should be available rsn __eou__ __eot__ should g2 in ubuntu do the magic dont-focus-window tricks? __eou__ join the gang, get an x-series thinkpad __eou__ sj has hung on my box, again. __eou__ what is monday mornings discussion actually about? __eou__ __eot__ ", "interesting __eou__ grub-install worked with / being ext3, failed when it was xfs __eou__ i thought d-i installed the relevant kernel for your machine. i have a p4 and its installed the 386 kernel __eou__ holy crap a lot of stuff gets installed by default :) __eou__ YOU ARE INSTALLING VIM ON A BOX OF MINE __eou__ ;) __eou__ __eot__ more like osx than debian ;) __eou__ we have a selection of python modules available for great justice (and python development) __eou__ __eot__ 2.8 is fixing them iirc __eou__ __eot__ pong __eou__ vino will be in __eou__ enjoying ubuntu? __eou__ __eot__ told me to come here __eou__ suggested thursday as a good day to come __eou__ __eot__ we froze versions a while back :) __eou__ you coming today or thursday? __eou__ we're considering shifting it __eou__ yay __eou__ enjoying ubuntu? __eou__ usplash! __eou__ __eot__ thats the one __eou__ __eot__ so i saw your email with the mockup at the airport, but it hasn't appeared now that i've pulled my mail :| __eou__ __eot__ i've got a better one now too, give me a minute __eou__ we've got rh9 installed on most desktops. you want me to look at up2date, right? __eou__ __eot__ aha! no, the gui thingy __eou__ it's more wizardy __eou__ so the first page is okayish __eou__ we can do a whole load better on the second page (icons, translated descriptions) __eou__ but that's the kind of thing i was thinking about __eou__ (a single big treeview would get very scary, very quickly) __eou__ sure it's not a hurricane? __eou__ __eot__ i think experimental is getting 2.8 too __eou__ let him work on #1217 :) __eou__ __eot__ we call it 'universe' ;) __eou__ haha __eou__ ooh, totally __eou__ __eot__ i want it on in sarge too but nobody else agrees __eou__ __eot__ ", 'and because Python gives Mark a woody __eou__ __eot__ i\'m not sure if we\'re meant to talk about that publically yet. __eou__ __eot__ and I thought we were a "pants off" kind of company ... :p __eou__ you need new glasses __eou__ __eot__ mono 1.0? dude, that\'s going to be a barrel of laughs for totally non-release related reasons during hoary __eou__ read bryan clark\'s entry about NetworkManager? __eou__ __eot__ there was an accompanying IRC conversation to that one <g> __eou__ explain ? __eou__ I guess you could ship the new png in the debian/ directory and copy them over in your rules __eou__ __eot__ but debian/ is also part of diff.gz... __eou__ you can fix this for the common people, dude! multiple tarballs in source! __eou__ __eot__ NOTWARTY, HTH, HAND, KTHXBYE <g> __eou__ everyone else had their macs stolen, so can\'t really comment __eou__ that picture of you is a classic __eou__ __eot__ which? __eou__ the best feature of the new imac is that the old imacs are going to be cheaper! __eou__ ooh, can you add that to the wiki? __eou__ __eot__ k. __eou__ you getting two-weeks-to-release edginess? __eou__ http://descent.netsplit.com/~scott/kids.mp3 -- but for releases __eou__ I played with an x300 about the time I bought my new laptop, it didn\'t feel solid at all __eou__ __eot__ which series is yours again? __eou__ nc8000? __eou__ mmm __eou__ __eot__ you have my sympathy __eou__ I\'m trying to *find* the definition I wrote __eou__ __eot__ it\'d be on Glossary __eou__ i know i wrote one there __eou__ __eot__ I\'m trying to find the one with mdz\'s l33t dot madness __eou__ that would be a pretty good look for you :p __eou__ bandwidth bills? __eou__ __eot__ i\'m reverting the wifi change; i don\'t think the bars are the right thing, but they\'re better than the current one. __eou__ ooh, that\'d be rad __eou__ __eot__ not about waiting?  clearly you haven\'t tried to read a site that\'s just made slashdot? __eou__ __eot__ ']
['basically each xfree86 upload will NOT force users to upgrade 100Mb of fonts for nothing __eou__ no something i did in my spare time. __eou__', 'oh? oops. __eou__', "we'll have a BOF about this __eou__ so you're coming tomorrow ? __eou__", 'i fully endorse this suggestion </quimby> __eou__ how did your reinstall go? __eou__', "(i thought someone was going to make a joke about .au bandwidth...) __eou__ especially not if you're using screen ;) __eou__"]

train loss: 1.7207267924880982 / valid loss: 1.4168381658298879 -------------------- epoch: 0 iteration: 6250 ==> save
train loss: 1.4005630728244782 / valid loss: 1.2224228949135143 -------------------- epoch: 0 iteration: 12500 ==> save
train loss: 1.2675697498750687 / valid loss: 1.1147578920553243 -------------------- epoch: 0 iteration: 18750 ==> save
train loss: 1.1879680690288543 / valid loss: 1.0834381742046235 -------------------- epoch: 0 iteration: 25000 ==> save
train loss: 1.1549719080233574 / valid loss: 1.0344085222302208 -------------------- epoch: 0 iteration: 31250 ==> save
scheduler!
train loss: 1.11702395996809 / valid loss: 0.9890826257058525 -------------------- epoch: 0 iteration: 37500 ==> save
train loss: 1.0833061012578011 / valid loss: 0.9875307299818618 -------------------- epoch: 0 iteration: 43750 ==> save
train loss: 1.063168302938938 / valid loss: 0.9777263748153142 -------------------- epoch: 0 iteration: 50000 ==> save
train loss: 1.0392135172843933 / valid loss: 0.968436890740539 -------------------- epoch: 0 iteration: 56250 ==> save
train loss: 1.0248301701033116 / valid loss: 0.9657115194391697 -------------------- epoch: 0 iteration: 62500 ==> save
scheduler!
train loss: 0.9336123527109623 / valid loss: 0.9300745850864877 -------------------- epoch: 1 iteration: 6250 ==> save
train loss: 0.9307834273457527 / valid loss: 0.9483699315942445 -------------------- epoch: 1 iteration: 12500
train loss: 0.937566038287878 / valid loss: 0.9321518669883475 -------------------- epoch: 1 iteration: 18750
train loss: 0.9346756827497482 / valid loss: 0.9349656995940716 -------------------- epoch: 1 iteration: 25000
train loss: 0.9367433539283275 / valid loss: 0.9072045526069432 -------------------- epoch: 1 iteration: 31250 ==> save
scheduler!
train loss: 0.9298626047182084 / valid loss: 0.9495086028757345 -------------------- epoch: 1 iteration: 37500
train loss: 0.9240250580000877 / valid loss: 0.9163055106960908 -------------------- epoch: 1 iteration: 43750
train loss: 0.9268160011816025 / valid loss: 0.8884774011402786 -------------------- epoch: 1 iteration: 50000 ==> save
train loss: 0.9215158633112908 / valid loss: 0.8941724410375089 -------------------- epoch: 1 iteration: 56250
train loss: 0.9197544271564484 / valid loss: 0.8909589532951774 -------------------- epoch: 1 iteration: 62500
scheduler!
train loss: 0.8090048681747913 / valid loss: 0.895540458261479 -------------------- epoch: 2 iteration: 6250
train loss: 0.8280719810831547 / valid loss: 0.8912334231790856 -------------------- epoch: 2 iteration: 12500
train loss: 0.8379328211647272 / valid loss: 0.8758233313352699 -------------------- epoch: 2 iteration: 18750 ==> save
train loss: 0.8395111404597759 / valid loss: 0.8764030377025397 -------------------- epoch: 2 iteration: 25000
train loss: 0.8447131197106839 / valid loss: 0.8486022607235881 -------------------- epoch: 2 iteration: 31250 ==> save
scheduler!
train loss: 0.8388366147816181 / valid loss: 0.8831598428177073 -------------------- epoch: 2 iteration: 37500
train loss: 0.8408704332494735 / valid loss: 0.8694475670810027 -------------------- epoch: 2 iteration: 43750
train loss: 0.8473843506801129 / valid loss: 0.868401734618723 -------------------- epoch: 2 iteration: 50000
train loss: 0.8486349509978295 / valid loss: 0.8644059115499502 -------------------- epoch: 2 iteration: 56250
train loss: 0.8394645369821787 / valid loss: 0.8831909607102068 -------------------- epoch: 2 iteration: 62500
scheduler!
train loss: 0.736921037594676 / valid loss: 0.8562152116954229 -------------------- epoch: 3 iteration: 6250
train loss: 0.7448304996222258 / valid loss: 0.8616313734953025 -------------------- epoch: 3 iteration: 12500
train loss: 0.7621072409641743 / valid loss: 0.8910326889903573 -------------------- epoch: 3 iteration: 18750
train loss: 0.7663840144711733 / valid loss: 0.874591844488673 -------------------- epoch: 3 iteration: 25000
train loss: 0.778537666631937 / valid loss: 0.8501524386619586 -------------------- epoch: 3 iteration: 31250
scheduler!
train loss: 0.7811190440797806 / valid loss: 0.8352377259228897 -------------------- epoch: 3 iteration: 37500 ==> save
train loss: 0.7902187310761213 / valid loss: 0.8521088267540873 -------------------- epoch: 3 iteration: 43750
train loss: 0.7944794798469543 / valid loss: 0.8753359142367754 -------------------- epoch: 3 iteration: 50000
train loss: 0.7959634763133526 / valid loss: 0.8534090902820937 -------------------- epoch: 3 iteration: 56250
train loss: 0.7938160848116874 / valid loss: 0.8554141013753492 -------------------- epoch: 3 iteration: 62500
scheduler!
train loss: 0.6921091105744243 / valid loss: 0.85294547691673 -------------------- epoch: 4 iteration: 6250
train loss: 0.7032245805174112 / valid loss: 0.8785962323900526 -------------------- epoch: 4 iteration: 12500
train loss: 0.7133779505684972 / valid loss: 0.849328721490873 -------------------- epoch: 4 iteration: 18750
train loss: 0.7257792405951023 / valid loss: 0.874139801027488 -------------------- epoch: 4 iteration: 25000
train loss: 0.7302923638606071 / valid loss: 0.8570792637856863 -------------------- epoch: 4 iteration: 31250
scheduler!
train loss: 0.7411285282361507 / valid loss: 0.8517798112793757 -------------------- epoch: 4 iteration: 37500
train loss: 0.7355740404507518 / valid loss: 0.8454468547139444 -------------------- epoch: 4 iteration: 43750
train loss: 0.7518537612295151 / valid loss: 0.8596142581961257 -------------------- epoch: 4 iteration: 50000
train loss: 0.7449865589928627 / valid loss: 0.8527900712890779 -------------------- epoch: 4 iteration: 56250
train loss: 0.7578094640034437 / valid loss: 0.8419063619342548 -------------------- epoch: 4 iteration: 62500
scheduler!
END!! 2022_10_05 / 09_37
RUNNING TIME: 1 day, 
============================================================



SCORE!!
Namespace(best='0', device='cuda', lang='en', m=0, model='bi', path='bi221003_1959_bs16_ep5_data21000000_en_best0', task='ubuntu2', testset='ubuntu2_test_18920.pickle')
Load BiEncoder
bi221003_1959_bs16_ep5_data21000000_en_best0
R@1/10: 78.97
MRR: 86.88
============================================================

SCORE!!
Namespace(best='1', device='cuda', lang='en', m=0, model='bi', path='bi221003_1959_bs16_ep5_data21000000_en_best1', task='ubuntu2', testset='ubuntu2_test_18920.pickle')
Load BiEncoder
bi221003_1959_bs16_ep5_data21000000_en_best1
R@1/10: 78.81
MRR: 86.91
============================================================

